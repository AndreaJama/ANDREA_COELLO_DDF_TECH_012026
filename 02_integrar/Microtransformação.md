## ‚≠ê B√¥nus ‚Äì Microtransforma√ß√£o com PostgreSQL (Supabase)

Como parte do b√¥nus do case t√©cnico, foi implementada uma pipeline de ingest√£o de dados a partir de um **banco de dados PostgreSQL em ambiente cloud (Supabase)**, simulando uma **base transacional real**, utilizada como sistema de origem.

O dataset utilizado cont√©m registros de viagens da **NYC Taxi & Limousine Commission (TLC)** referentes a janeiro de 2024, previamente tratados e carregados no PostgreSQL para garantir consist√™ncia de tipos e formatos antes da ingest√£o.

---

### üîπ Arquitetura adotada

A solu√ß√£o segue o paradigma **ELT (Extract, Load, Transform)**:

- **Extract**: leitura de dados a partir de uma base PostgreSQL transacional  
- **Load**: ingest√£o dos dados no Data Lake da Dadosfera  
- **Transform**: aplica√ß√£o de microtransforma√ß√µes durante o processo de coleta  

Essa abordagem permite maior flexibilidade anal√≠tica, al√©m de refor√ßar pr√°ticas de **governan√ßa, seguran√ßa e rastreabilidade dos dados**.

---

### üîπ Configura√ß√£o da fonte PostgreSQL

A fonte de dados foi cadastrada na Dadosfera com os seguintes par√¢metros:

- **Banco de dados**: PostgreSQL (Supabase ‚Äì Cloud)
- **Tipo de conex√£o**: Direta
- **Endpoint**: via Transaction Pooler (IPv4 compat√≠vel)
- **Porta**: 6543
- **Database**: `postgres`
- **Usu√°rio**: usu√°rio padr√£o do Supabase
- **Schema coletado**: `public`\
  
![conf_postgreSQL](conf_postgreSQL.png)

---

### üîπ Configura√ß√£o da pipeline

Na cria√ß√£o da pipeline:

- Foi selecionado o objeto **`nyc_taxi_trips_2024_01`**
- Modo de sincroniza√ß√£o configurado como **Full Load**

![info_pipeline_postgresql](info_pipeline_postgresql.png)

**Justificativa**:
- Dataset hist√≥rico
- Base est√°tica para an√°lise explorat√≥ria e anal√≠tica
- Simplifica√ß√£o do controle de carga para o escopo do case

![pipeline_schema](pipeline_schema.png)

![sel_objeto](sel_objeto.png)

![fullload](fullload.png)

---

### üîπ Microtransforma√ß√£o aplicada (Hash)

Durante a configura√ß√£o da pipeline, foi aplicada uma **microtransforma√ß√£o do tipo _Criptografar (Hash)_** sobre a coluna:

- **Coluna**: `VendorID`
- **Objetivo**:
  - Anonimizar identificadores sens√≠veis
  - Preservar consist√™ncia entre valores repetidos
  - Permitir an√°lises agregadas sem exposi√ß√£o de dados sens√≠veis

A microtransforma√ß√£o √© aplicada **evento a evento** durante a ingest√£o, garantindo que valores iguais resultem no mesmo hash.

![microtransformacao](microtransformacao.png)

---

### üîπ Agendamento e execu√ß√£o

A pipeline foi configurada como **√önica extra√ß√£o**, executando a coleta imediatamente ap√≥s a cria√ß√£o.

Essa escolha foi feita por se tratar de um **dataset est√°tico**, adequado ao contexto de demonstra√ß√£o t√©cnica do case, mantendo a possibilidade de reexecu√ß√£o manual se necess√°rio.

![unica_extracao](unica_extracao.png)

---

### üîπ Resultado final

Ap√≥s a execu√ß√£o da pipeline:

- Os dados foram ingeridos com sucesso na Dadosfera
- O dataset foi automaticamente **catalogado como Data Asset**
- A coluna `VendorID` passou a apresentar valores **anonimizados (hash)**
- As demais colunas permaneceram intactas, garantindo qualidade e integridade dos dados

![pipeline_criada](pipeline_criada.png)

![pipeline_at](pipeline_at.png)

---

### üîπ Considera√ß√µes finais

Este b√¥nus demonstra, de forma pr√°tica:

- Integra√ß√£o entre banco transacional cloud e Data Lake
- Aplica√ß√£o de **boas pr√°ticas de ELT**
- Uso de microtransforma√ß√µes para **seguran√ßa e privacidade**
- Capacidade da plataforma em realizar ingest√£o, transforma√ß√£o e cataloga√ß√£o de forma integrada

O processo refor√ßa uma vis√£o orientada a **governan√ßa de dados**, **qualidade** e **escalabilidade**, alinhada a cen√°rios reais de produ√ß√£o.
