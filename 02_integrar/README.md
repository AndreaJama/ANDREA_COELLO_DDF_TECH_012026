# Item 2.1 ‚Äì Integra√ß√£o de Dados na Dadosfera

Nesta etapa, foi realizado o carregamento da base de dados proposta para a plataforma Dadosfera, utilizando o m√≥dulo de Coleta.

## Fonte de Dados
Base p√∫blica da NYC Taxi & Limousine Commission (TLC), contendo milh√µes de registros de viagens, reinterpretadas neste projeto como entregas log√≠sticas no contexto de e-commerce.

## Prepara√ß√£o do Arquivo para Ingest√£o

Antes da importa√ß√£o dos dados na Dadosfera, foi realizada uma valida√ß√£o b√°sica da base de dados original em formato Parquet, incluindo verifica√ß√£o de volume e estrutura.

Para facilitar a ingest√£o manual na plataforma, foi gerado um arquivo CSV a partir de uma amostra de 200.000 registros, mantendo o dado bruto sem transforma√ß√µes anal√≠ticas.

O script utilizado para essa prepara√ß√£o encontra-se em:
- `scripts/prepare_nyc_taxi_data.py`

## M√©todo de Coleta
Foi utilizada a importa√ß√£o manual de arquivos CSV, uma vez que se trata de um dataset est√°tico para fins de an√°lise e desenvolvimento do case.

### Importa√ß√£o Manual de Arquivos

Nesta etapa, os dados foram carregados na plataforma Dadosfera por meio da funcionalidade **Importar arquivos**, recomendada para bases de dados est√°ticas ou que n√£o necessitam de atualiza√ß√£o recorrente.

<img src="importar.png" alt="importar" width="400"/>

<img src="novo_arquivo.png" alt="novo_arquivo" width="600"/>

A origem dos dados √© o dispositivo local, caracterizando uma coleta √∫nica, sem agendamento autom√°tico.

### Configura√ß√µes da Importa√ß√£o

O arquivo importado segue as especifica√ß√µes suportadas pela plataforma, conforme detalhado abaixo:

| Configura√ß√£o            | Valor Utilizado |
|-------------------------|-----------------|
| Tipo de arquivo         | CSV |
| Codifica√ß√£o             | UTF-8 |
| Separador               | `,` |
| Cabe√ßalho               | Sim |
| Tamanho do arquivo      | Inferior a 250 MB |

<img src="configuracoes.png" alt="configuracoes" width="800"/>

Durante o processo de importa√ß√£o, foi definido um nome e uma descri√ß√£o para o dataset, permitindo melhor contextualiza√ß√£o e governan√ßa dos dados no cat√°logo da plataforma.

<img src="informacoes.png" alt="informacoes" width="800"/>

<img src="arquivo_carregado.png" alt="arquivo_carregado" width="800"/>

## Acompanhar importa√ß√£o
Verifique o Status da extra√ß√£o do arquivo do seu dispositivo:

<img src="status1.png" alt="status1" width="400"/>

<img src="final_status.png" alt="final_status" width="600"/>

Ap√≥s a conclus√£o da importa√ß√£o, o dataset ficou dispon√≠vel para visualiza√ß√£o e consulta no **Cat√°logo de Dados** da Dadosfera.

[Cat√°logo](https://app.dadosfera.ai/pt-BR/catalog/data-assets/34bfb383-60a8-4e70-af64-dc9d53f595f7)

## ‚≠ê B√¥nus ‚Äì Microtransforma√ß√£o com PostgreSQL (Supabase)
 
A Dadosfera adota o paradigma ELT, permitindo a aplica√ß√£o de microtransforma√ß√µes no momento da ingest√£o de dados provenientes de fontes transacionais SQL.

No contexto deste projeto, a microtransforma√ß√£o de criptografia (hash) seria aplicada sobre a coluna `VendorID`, com o objetivo de anonimizar identificadores sens√≠veis mantendo a consist√™ncia anal√≠tica dos dados.

Devido ao uso de importa√ß√£o manual de arquivos CSV (coleta √∫nica), a aplica√ß√£o pr√°tica da microtransforma√ß√£o n√£o foi executada na interface, sendo o conceito, a escolha da coluna e o impacto documentados conforme as boas pr√°ticas da plataforma.

---

### üîπ Arquitetura adotada

A solu√ß√£o segue o paradigma **ELT (Extract, Load, Transform)**:

- **Extract**: leitura de dados a partir de uma base PostgreSQL transacional  
- **Load**: ingest√£o dos dados no Data Lake da Dadosfera  
- **Transform**: aplica√ß√£o de microtransforma√ß√µes durante o processo de coleta  

Essa abordagem permite maior flexibilidade anal√≠tica, al√©m de refor√ßar pr√°ticas de **governan√ßa, seguran√ßa e rastreabilidade dos dados**.

---

### üîπ Configura√ß√£o da fonte PostgreSQL

A fonte de dados foi cadastrada na Dadosfera com os seguintes par√¢metros:

- **Banco de dados**: PostgreSQL (Supabase ‚Äì Cloud)
- **Tipo de conex√£o**: Direta
- **Endpoint**: via Transaction Pooler (IPv4 compat√≠vel)
- **Porta**: 6543
- **Database**: `postgres`
- **Usu√°rio**: usu√°rio padr√£o do Supabase
- **Schema coletado**: `public`\
  
![conf_postgreSQL](conf_postgreSQL.png)

---

### üîπ Configura√ß√£o da pipeline

Na cria√ß√£o da pipeline:

- Foi selecionado o objeto **`nyc_taxi_trips_2024_01`**
- Modo de sincroniza√ß√£o configurado como **Full Load**

![info_pipeline_postgresql](info_pipeline_postgresql.png)

**Justificativa**:
- Dataset hist√≥rico
- Base est√°tica para an√°lise explorat√≥ria e anal√≠tica
- Simplifica√ß√£o do controle de carga para o escopo do case

![pipeline_schema](pipeline_schema.png)

![sel_objeto](sel_objeto.png)

![fullload](fullload.png)

---

### üîπ Microtransforma√ß√£o aplicada (Hash)

Durante a configura√ß√£o da pipeline, foi aplicada uma **microtransforma√ß√£o do tipo _Criptografar (Hash)_** sobre a coluna:

- **Coluna**: `VendorID`
- **Objetivo**:
  - Anonimizar identificadores sens√≠veis
  - Preservar consist√™ncia entre valores repetidos
  - Permitir an√°lises agregadas sem exposi√ß√£o de dados sens√≠veis

A microtransforma√ß√£o √© aplicada **evento a evento** durante a ingest√£o, garantindo que valores iguais resultem no mesmo hash.

![microtransformacao](microtransformacao.png)

---

### üîπ Agendamento e execu√ß√£o

A pipeline foi configurada como **√önica extra√ß√£o**, executando a coleta imediatamente ap√≥s a cria√ß√£o.

Essa escolha foi feita por se tratar de um **dataset est√°tico**, adequado ao contexto de demonstra√ß√£o t√©cnica do case, mantendo a possibilidade de reexecu√ß√£o manual se necess√°rio.

![unica_extracao](unica_extracao.png)

---

### üîπ Resultado final

Ap√≥s a execu√ß√£o da pipeline:

- Os dados foram ingeridos com sucesso na Dadosfera
- O dataset foi automaticamente **catalogado como Data Asset**
- A coluna `VendorID` passou a apresentar valores **anonimizados (hash)**
- As demais colunas permaneceram intactas, garantindo qualidade e integridade dos dados

![pipeline_criada](pipeline_criada.png)

![pipeline_at](pipeline_at.png)

[Pipeline](https://app.dadosfera.ai/pt-BR/collect/pipelines/c053bd76-e050-4527-8bb7-15f6d735344f)
---

### üîπ Considera√ß√µes finais

Este b√¥nus demonstra, de forma pr√°tica:

- Integra√ß√£o entre banco transacional cloud e Data Lake
- Aplica√ß√£o de **boas pr√°ticas de ELT**
- Uso de microtransforma√ß√µes para **seguran√ßa e privacidade**
- Capacidade da plataforma em realizar ingest√£o, transforma√ß√£o e cataloga√ß√£o de forma integrada

O processo refor√ßa uma vis√£o orientada a **governan√ßa de dados**, **qualidade** e **escalabilidade**, alinhada a cen√°rios reais de produ√ß√£o.



